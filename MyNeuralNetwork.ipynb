{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBn1w5DaFJEZ7L7BGMcKd+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uyan4LlsFVGB"},"outputs":[],"source":["#********************************************** NETWORK **********************************************\n","#********************************************** NETWORK **********************************************\n","#********************************************** NETWORK **********************************************\n","import numpy as np\n","\n","\n","def initialize_parameters_deep(layer_dims):\n","    np.random.seed(1)\n","    parameters = {}\n","    L = len(layer_dims)\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1])/np.sqrt(layer_dims[l-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","\n","        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","    return parameters\n","\n","\n","def linear_forward(A, W, b):\n","    Z = W.dot(A) + b\n","\n","    assert (Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","\n","    return Z, cache\n","\n","\n","def relu(Z):\n","    A = np.maximum(0,Z)\n","    assert(A.shape == Z.shape)\n","    cache = Z\n","    return A, cache\n","\n","\n","def linear(Z):\n","    cache = Z\n","    return Z, cache\n","\n","\n","def sigmoid(Z):\n","    A = 1/(1 + np.exp(-Z))\n","    cache = Z\n","    return A, cache\n","\n","\n","def softmax(Z):\n","  expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n","  A = expZ / np.sum(expZ, axis=0, keepdims=True)\n","  cache = Z\n","  return A, cache\n","\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","\n","    Z, linear_cache = linear_forward(A_prev, W, b)\n","    if activation == \"relu\":\n","        A, activation_cache = relu(Z)\n","    elif activation == \"linear\":\n","        A, activation_cache = linear(Z)\n","    elif activation == \"sigmoid\":\n","        A, activation_cache = sigmoid(Z)\n","    elif activation == \"softmax\":\n","        A, activation_cache = softmax(Z)\n","\n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache\n","\n","\n","def L_model_forward(X, parameters):\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2\n","\n","    for l in range(1, L):\n","        A_prev = A\n","        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n","        caches.append(cache)\n","\n","    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n","    caches.append(cache)\n","\n","    assert (AL.shape == (parameters['W' + str(L)].shape[0], X.shape[1]))\n","\n","    return AL, caches\n","\n","\n","# Mean Square Error\n","def Mean_Square_Error(AL, Y, parameters, lambd):\n","    m = Y.shape[1]\n","    cost_mse = (1 / m) * np.sum((AL - Y) ** 2)\n","\n","    L2_regularization_cost = 0\n","    L = len(parameters) // 2\n","    for l in range(1, L + 1):\n","        L2_regularization_cost += (1 / m) * (lambd / 2) * np.sum(np.square(parameters['W' + str(l)]))\n","\n","    cost = cost_mse + L2_regularization_cost\n","    cost = np.squeeze(cost)\n","    assert (cost.shape == ())\n","\n","    return cost\n","\n","\n","# Binary Cross Entropy\n","def Binary_Cross_Entropy(AL, Y, parameters, lambd):\n","    m = Y.shape[1]\n","\n","    eps = 1e-10\n","    BCE_cost = (-1/m) * (np.dot(Y, np.log(AL + eps).T) + np.dot((1 - Y), np.log(1 - AL + eps).T))\n","\n","    L2_regularization_cost = 0\n","    L = len(parameters) // 2\n","    for l in range(1, L + 1):\n","        L2_regularization_cost += (1 / m) * (lambd / 2) * np.sum(np.square(parameters['W' + str(l)]))\n","\n","    cost = BCE_cost + L2_regularization_cost\n","    print(cost.shape)\n","    cost = np.squeeze(cost)\n","    assert (cost.shape == ())\n","\n","    return cost\n","\n","\n","# Cross Entropy Loss\n","def Cross_Entropy_Loss(AL, Y, parameters, lambd):\n","    eps = 1e-10\n","    m = Y.shape[1]\n","    cross_entropy_cost = (-1 / m) * np.sum(Y * np.log(AL + eps))\n","\n","    L2_regularization_cost = 0\n","    L = len(parameters) // 2\n","    for l in range(1, L + 1):\n","        L2_regularization_cost += (1 / m) * (lambd / 2) * np.sum(np.square(parameters['W' + str(l)]))\n","\n","    cost = cross_entropy_cost + L2_regularization_cost\n","    cost = np.squeeze(cost)\n","    assert (cost.shape == ())\n","\n","    return cost\n","\n","\n","def compute_cost(AL, Y, parameters, lambd, function):\n","    if function == \"MSE\":\n","        cost = Mean_Square_Error(AL, Y, parameters, lambd)\n","    elif function == \"BCE\":\n","        cost = Binary_Cross_Entropy(AL, Y, parameters, lambd)\n","    elif function == \"CEL\":\n","        cost = Cross_Entropy_Loss(AL, Y, parameters, lambd)\n","\n","    return cost\n","\n","\n","def linear_backward(dZ, cache):\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = (1/m)*np.dot(dZ, A_prev.T)\n","    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","\n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","\n","    return dA_prev, dW, db\n","\n","\n","def relu_backward(dA, activation_cache):\n","    Z = activation_cache\n","    dZ = np.array(dA, copy=True)\n","\n","    dZ[Z <= 0] = 0\n","    assert (dZ.shape == Z.shape)\n","\n","    return dZ\n","\n","\n","def sigmoid_backward(dA, activation_cache):\n","    Z = activation_cache\n","    s = 1/(1 + np.exp(-Z))\n","\n","    dZ = dA*s*(1-s)\n","    assert (dZ.shape == Z.shape)\n","\n","    return dZ\n","\n","\n","def softmax_backward(dA, activation_cache):\n","    # Z = activation_cache\n","    # s = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n","    # s = s / np.sum(s, axis=0, keepdims=True)\n","    # dZ = dA - s\n","    # dZ = dA\n","    # assert (dZ.shape == Z.shape)\n","\n","    return dA\n","\n","\n","def linear_activation_backward(dA, cache, activation):\n","    linear_cache, activation_cache = cache\n","\n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","    elif activation == \"linear\":\n","        dZ = dA\n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","    elif activation == \"softmax\":\n","        dZ = softmax_backward(dA, activation_cache)\n","\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    return dA_prev, dW, db\n","\n","\n","def L_model_backward(AL, Y, caches):\n","    grads = {}\n","    L = len(caches)\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape)\n","\n","    dAL = AL - Y\n","\n","    current_cache = caches[L-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\")\n","\n","    for l in reversed(range(L-1)):\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads\n","\n","\n","def update_parameters(parameters, grads, learning_rate):\n","\n","    L = len(parameters) // 2\n","\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","\n","    return parameters\n","\n","\n","\n","\n","\n","#********************************************** GO USE NETWORK **********************************************\n","#********************************************** GO USE NETWORK **********************************************\n","#********************************************** GO USE NETWORK **********************************************\n","\n","\n","\n","\"\"\" example of learning and using ! \"\"\"\n","\n","\n","\n","# TRAINING\n","def training(model, X, Y, learning_rate, num_epochs, batch_size, loss_function, lambd, step__output__cost, flag__output):\n","  parameters = initialize_parameters_deep(model)\n","  m = X.shape[1]\n","\n","  for i in range(num_epochs):\n","    # mix data\n","    permutation = np.random.permutation(m) # mix indexes\n","    X_shuffled = X[:, permutation]\n","    Y_shuffled = Y[:, permutation]\n","\n","    # train by batch\n","    for start in range(0, m, batch_size):\n","      end = min(start + batch_size, m)\n","      X_batch = X_shuffled[:, start:end]\n","      Y_batch = Y_shuffled[:, start:end]\n","\n","      AL, caches = L_model_forward(X_batch, parameters)\n","      cost = compute_cost(AL, Y_batch, parameters, lambd, loss_function)\n","      grads = L_model_backward(AL, Y_batch, caches)\n","      parameters = update_parameters(parameters, grads, learning_rate)\n","\n","\n","    if i % step__output__cost == 0 and flag__output:\n","      print(f\"Cost after epoch {i}: {cost}\")\n","\n","  return parameters\n","\n","\n","# PREDICTION\n","def predict(X, parameters):\n","    X = np.array(X).T\n","    probas, caches = L_model_forward(X, parameters)\n","    return probas\n","\n","\n","\n","# PREPARE DATASETS FOR TRAIN\n","train_x = []\n","train_y = []\n","X = np.array(X).T\n","Y = np.eye(4)[Y].T\n","\n","\n","# HYPERPARAMETERS\n","input_neurons = 2\n","output_neurons = 4\n","model = [input_neurons, 64, 64, 64, output_neurons]\n","learning_rate = 0.045\n","num_epochs = 5000\n","batch_size = 32\n","loss_function = \"CEL\"\n","lambd = 0\n","step__output__cost = int(num_epochs / 10)\n","flag__output = True\n","\n","# GO TRAIN PARAMETERS\n","trained_parameters = training(model, train_x, train_y, learning_rate, num_epochs, batch_size, loss_function, lambd, step__output__cost, flag__output)\n","\n","\n","# PREPARE DATASETS FOR TEST\n","test_x = []\n","test_y = []\n","\n","# GO PREDICT\n","predictions = predict(test_x, trained_parameters)"]}]}
